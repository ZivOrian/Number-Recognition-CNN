import torch
from torch import autograd as grad
import torch.nn as nn
import tensorflow as tf
import time
import matplotlib.pyplot as plt

# Generate data (train and test set)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)

W1 = torch.rand([784, 784*20], requires_grad=True,dtype=float)  # Set requires_grad for gradient calculation
nn.init.xavier_uniform_(W1)
W2 = torch.rand([784*20, 10], requires_grad=True,dtype=float)
nn.init.xavier_uniform_(W2)

# "for loop" Segment

start_time = time.time()

#    -----Init-----
learn_rate = 0.01
x_i = torch.tensor(x_train[0],dtype=float)
x_i = torch.flatten(x_i)
y = y_train[0]
y = [1 if i == y-1 else 0 for i in range(10)]
y = torch.tensor(y,dtype=float)

# --Forward Propagation--
h1_layer = torch.matmul(torch.t(x_i), W1)
h1_layer = nn.functional.relu(h1_layer)
out_layer = torch.matmul(torch.t(h1_layer), W2)
y_hat = nn.functional.softmax(out_layer, dim=0)

# --Backward Propagtion--
loss = nn.CrossEntropyLoss()  # Instantiate the loss function
loss_value = loss(y_hat, y)  # Calculate loss with predicted probabilities and labels
loss_value.backward()

wgrad = W2.grad
W1 = torch.sub(W1, W1.grad, alpha=learn_rate)
W2 = torch.sub(W2, W2.grad, alpha=learn_rate)


# --Test--
torch.set_printoptions(10)
print(time.time() - start_time,end='\n\nye\n\n ')
print(wgrad.sum())

